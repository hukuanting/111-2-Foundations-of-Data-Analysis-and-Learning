{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-hyv-8pA0J"
      },
      "source": [
        "# HW3: Document retrieval of the AICUP2023 competition\n",
        "The goal of this homework is to implement a document retrieval system for the AICUP2023 competition using the [`TF-IDF`](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJkYUTXIpA0L"
      },
      "source": [
        "## What is TF-IDF?\n",
        "- TF: Term Frequency, the occurring time ($n$) of `a term` $t$ appears in `a document` $d$.\n",
        "$$\\textrm{TF}_{t,d} =  \\frac{n_{t,d}}{\\sum_k n_{k,d}}$$\n",
        ", where $\\sum_k n_{k,d}$ indicates summation of all terms in the document $d$.\n",
        "- IDF: Inverse Document Frequency, how frequent `a term` $t$ appears in all documents.\n",
        "    - Document Frequency $\\textrm{DF}_t = \\log\\frac{|\\{d: t\\in d\\}|}{|D|}$, where $|\\{d: t\\in d\\}|$ is the number of documents that `a term` t appears, and $|D|$ indicates total number of documents.\n",
        "    - $\\Rightarrow \\textrm{IDF}_t = \\log\\frac{|D|}{|\\{d: t\\in d\\}|}$\n",
        "- $\\textrm{TFIDF}_{t,d} = \\textrm{TF}_{t,d} \\times \\textrm{IDF}_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0u6terNpA0M"
      },
      "source": [
        "## What can TF-IDF do?\n",
        "- TF-IDF can be used to transform a document or a sentence into a `vector`.\n",
        "- There are other ways to transform a document or a sentence into a vector, such as [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html), [`BERT`](https://huggingface.co/docs/transformers/training), [`Sentence-BERT`](https://github.com/UKPLab/sentence-transformers), etc.\n",
        "- **This homework only asks you to implement the TF-IDF method.** You can try other methods in your final project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cQ73B1hpA0M"
      },
      "source": [
        "## Example of TF-IDF using `scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvJL15IopA0M"
      },
      "source": [
        "### Example 1: Use [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) + [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Assume we have 4 documents in a list\n",
        "corpus = [\n",
        "    \"this is the first document\",\n",
        "    \"this document is the second document\",\n",
        "    \"and this is the third one\",\n",
        "    \"is this the first document\",\n",
        "]\n",
        "# Assume we want some important words in the vocabulary\n",
        "vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n",
        "pipe = Pipeline(\n",
        "    [\n",
        "        (\"count\", CountVectorizer(vocabulary=vocabulary)),  # Get the count of each word\n",
        "        (\"tfidf\", TfidfTransformer(norm=None)),  # Get the tfidf of each word\n",
        "    ]\n",
        ").fit(corpus)\n",
        "\n",
        "# Print the count of each word\n",
        "print(pipe[\"count\"].transform(corpus).toarray())\n",
        ">>> [[1, 1, 1, 1, 0, 1, 0, 0], # count of the first document\n",
        "     [1, 2, 0, 1, 1, 1, 0, 0], # count of the second document\n",
        "     [1, 0, 0, 1, 0, 1, 1, 1], # count of the third document\n",
        "     [1, 1, 1, 1, 0, 1, 0, 0]] # count of the fourth document\n",
        "\n",
        "# Print the IDF values of each word\n",
        "# Notice!! The shape of the IDF values is (n_features, ).\n",
        "# n_features is the number of words in the vocabulary.\n",
        "print(pipe[\"tfidf\"].idf_)\n",
        ">>> [1.         1.22314355 1.51082562 1.         1.91629073 1.\n",
        " 1.91629073 1.91629073]\n",
        "\n",
        "# Print the TF-IDF values\n",
        "# Notice!! The shape of the TF-IDF values is (n_documents, n_features).\n",
        "print(pipe.transform(corpus).toarray())\n",
        ">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]\n",
        " [1.         2.4462871  0.         1.         1.91629073 1.\n",
        "  0.         0.        ]\n",
        " [1.         0.         0.         1.         0.         1.\n",
        "  1.91629073 1.91629073]\n",
        " [1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0zSO4j6pA0N"
      },
      "source": [
        "### Example 2: Use [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) alone\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assume we have 4 documents in a list\n",
        "corpus = [\n",
        "    \"this is the first document\",\n",
        "    \"this document is the second document\",\n",
        "    \"and this is the third one\",\n",
        "    \"is this the first document\",\n",
        "]\n",
        "# Assume we want some important words in the vocabulary\n",
        "vocabulary = [\"this\", \"document\", \"first\", \"is\", \"second\", \"the\", \"and\", \"one\"]\n",
        "vectorizer = TfidfVectorizer(\n",
        "    vocabulary=vocabulary,\n",
        "    norm=None, # without normalization\n",
        ")\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.toarray())\n",
        ">>> [[1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]\n",
        " [1.         2.4462871  0.         1.         1.91629073 1.\n",
        "  0.         0.        ]\n",
        " [1.         0.         0.         1.         0.         1.\n",
        "  1.91629073 1.91629073]\n",
        " [1.         1.22314355 1.51082562 1.         0.         1.\n",
        "  0.         0.        ]]\n",
        "```\n",
        "- Example 2 is the same as Example 1, but we use `TfidfVectorizer` instead of `CountVectorizer` + `TfidfTransformer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E52kKyJypA0O"
      },
      "source": [
        "## Why do we need to transform a document or a sentence into a vector?\n",
        "- Because we can use the `cosine similarity` to measure the similarity between each sentence and a Wikipedia article.\n",
        "- In our AICUP2023 task, we can find the most similar Wikipedia article for each `claim`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWgbosPFpA0O"
      },
      "source": [
        "## HW3 Delivery policies\n",
        "|Rules | Punishment | Note |\n",
        "|-|-|-|\n",
        "| Name the folder as `FDA_HW3_F12345678` and zip it | -5| |\n",
        "| Include `requirements.txt` in the folder | -5 | |\n",
        "| Use ChatGPT to predict the answers (not allowed in HW3) | -20 | You can use ChatGPT to help coding work. |\n",
        "| Results cannot be reproduced by re-running | -10 | We will run your code! |\n",
        "|Explicit rule-based corrections (e.g. predictions[0] = [\"天衛三\"]) | -20 |  |\n",
        "|Copy and paste the code from your classmates | -20 |  |\n",
        "|Modify the `do-not-modify cell` | -20 |  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MInCwBhppA0P"
      },
      "source": [
        "## HW3 Scoring rules\n",
        "|Rules | Scores|\n",
        "|-|-|\n",
        "| Perform document retrieval with scikit-learn `TF-IDF` by finishing all # TODO (each for +16) | +80 |\n",
        "| Document retrieval Precision >= 0.25 and Recall >= 0.6 (on the training set) | +10 |\n",
        "| Document retrieval Precision >= 0.4 and Recall >= 0.7 (on the training set) | +10 |\n",
        "| (Bonus) Document retrieval Precision >= 0.5 and Recall >= 0.7 (on the training set) | +10 |\n",
        "| (Bonus) Document retrieval Precision >= 0.6 and Recall >= 0.7 (on the training set) | +10 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wndpAB2pA0P"
      },
      "source": [
        "## Before you start\n",
        "1. Join the AICUP2023 competition. Link: https://tbrain.trendmicro.com.tw/Competitions/Details/28\n",
        "2. Download the training data and the Wikipedia data.\n",
        "3. (This code will need memory about 5.7 GB, tested on Google Colab.)\n",
        "\n",
        "### Folder structure\n",
        "- Please follow the following folder structure.\n",
        "```bash\n",
        "FDA_HW3_F12345678/\n",
        "│\n",
        "├── data/\n",
        "│ ├── public_train.jsonl # Downloaded from the AICUP2023 page\n",
        "│ └── wiki-pages # Downloaded from the AICUP2023 page and zipped\n",
        "│\n",
        "├── FDA2023_HW3.ipynb # This file\n",
        "├── hw3_utils.py\n",
        "└── requirements.txt\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jm9uA8vkHhBf"
      },
      "outputs": [],
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount ('/content/drive')\n",
        "!cp -r /content/drive/MyDrive/fda_2023_hw3_moodle/data /content\n",
        "!cp -r /content/drive/MyDrive/fda_2023_hw3_moodle/hw3_utils.py /content\n",
        "!cp -r /content/drive/MyDrive/fda_2023_hw3_moodle/requirements.txt /content'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ9zH11bpA0P",
        "outputId": "fe4c3f32-68d2-439a-89d9-0dfd532d7f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.9/dist-packages (1.6.4)\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from pandarallel) (0.3.6)\n",
            "Requirement already satisfied: pandas>=1 in /usr/local/lib/python3.9/dist-packages (from pandarallel) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from pandarallel) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1->pandarallel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1->pandarallel) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1->pandarallel) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: TCSP in /usr/local/lib/python3.9/dist-packages (0.0.9)\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the following lines if you use Google Colab\n",
        "!pip install pandarallel\n",
        "!pip install TCSP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWxGSSEbpA0Q"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from functools import partial\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jieba\n",
        "import scipy\n",
        "\n",
        "# jieba.set_dictionary(\"dict.txt.big\")\n",
        "# Download \"dict.txt.big\" from https://github.com/fxsjy/jieba\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "# Adjust the number of workers if you want\n",
        "pandarallel.initialize(progress_bar=True, verbose=0, nb_workers=4)\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas() # for progress_apply\n",
        "\n",
        "from hw3_utils import (\n",
        "    load_json,\n",
        "    jsonl_dir_to_df,\n",
        "    calculate_precision,\n",
        "    calculate_recall,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3rrNc7BpA0Q"
      },
      "outputs": [],
      "source": [
        "# Get the stopwords\n",
        "# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library\n",
        "from TCSP import read_stopwords_list\n",
        "\n",
        "stopwords = read_stopwords_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPwJv5qMpA0Q"
      },
      "outputs": [],
      "source": [
        "def tokenize(text: str, stopwords: list) -> str:\n",
        "    \"\"\"This function performs Chinese word segmentation and removes stopwords.\n",
        "\n",
        "    Args:\n",
        "        text (str): claim or wikipedia article\n",
        "        stopwords (list): common words that contribute little to the meaning of a sentence\n",
        "\n",
        "    Returns:\n",
        "        str: word segments separated by space (e.g. \"我 喜歡 吃 蘋果\")\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = jieba.cut(text)\n",
        "\n",
        "    return \" \".join([w for w in tokens if w not in stopwords])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42vbi-dUpA0R"
      },
      "outputs": [],
      "source": [
        "def get_pred_docs_sklearn(\n",
        "    claim: str,\n",
        "    tokenizing_method: callable,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    tf_idf_matrix: scipy.sparse.csr_matrix,\n",
        "    wiki_pages: pd.DataFrame,\n",
        "    topk: int,\n",
        ") -> set:\n",
        "\n",
        "    tokens = tokenizing_method(claim)\n",
        "    claim_vector = vectorizer.transform([tokens])\n",
        "    similarity_scores = cosine_similarity(tf_idf_matrix, claim_vector)\n",
        "\n",
        "    # `similarity_scores` shape: (num_wiki_pages x 1)\n",
        "    similarity_scores = similarity_scores[:, 0]  # flatten the array\n",
        "\n",
        "    # Sort the similarity scores in descending order\n",
        "    sorted_indices = sorted(range(len(similarity_scores)), key=lambda k: similarity_scores[k], reverse=True)\n",
        "    topk_sorted_indices = sorted_indices[:topk]\n",
        "\n",
        "    # Get the wiki page names based on the topk sorted indices\n",
        "    results = wiki_pages.iloc[topk_sorted_indices][\"id\"]\n",
        "\n",
        "    exact_matchs = []\n",
        "    # You can find the following code in our AICUP2023 baseline.\n",
        "    # Basically, we check if a result is exactly mentioned in the claim.\n",
        "    for result in results:\n",
        "        if (\n",
        "            (result in claim)\n",
        "            or (result in claim.replace(\" \", \"\")) # E.g., MS DOS -> MSDOS\n",
        "            or (result.replace(\"·\", \"\") in claim) # E.g., 湯姆·克魯斯 -> 湯姆克魯斯\n",
        "            or (result.replace(\"-\", \"\") in claim) # E.g., X-SAMPA -> XSAMPA\n",
        "        ):\n",
        "            exact_matchs.append(result)\n",
        "        elif \"·\" in result:\n",
        "            splitted = result.split(\"·\") # E.g., 阿爾伯特·愛因斯坦 -> 愛因斯坦\n",
        "            for split in splitted:\n",
        "                if split in claim:\n",
        "                    exact_matchs.append(result)\n",
        "                    break\n",
        "\n",
        "    return set(exact_matchs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pcwwwlJpA0R"
      },
      "outputs": [],
      "source": [
        "# Helper function (you don't need to modify this)\n",
        "\n",
        "def get_title_from_evidence(evidence):\n",
        "    titles = []\n",
        "    for evidence_set in evidence:\n",
        "        if len(evidence_set) == 4 and evidence_set[2] is None:\n",
        "            return [None]\n",
        "        for evidence_sent in evidence_set:\n",
        "            titles.append(evidence_sent[2])\n",
        "    return list(set(titles))\n",
        "\n",
        "\n",
        "def save_results_to_markdown(results: dict, output_file=\"grid_search_results.md\"):\n",
        "    file_exists = Path(output_file).exists()\n",
        "\n",
        "    with open(output_file, \"a\") as f:\n",
        "        if not file_exists:\n",
        "            f.write(\"# Grid Search Results\\n\\n\")\n",
        "            f.write(\"| Experiment  | F1 Score | Precision | Recall |\\n\")\n",
        "            f.write(\"| ----------- | -------- | --------- | ------ | \\n\")\n",
        "\n",
        "        exp_name = results[\"exp_name\"]\n",
        "        f1 = results[\"f1_score\"]\n",
        "        prec = results[\"precision\"]\n",
        "        recall = results[\"recall\"]\n",
        "        f.write(f\"| {exp_name} | {f1:.4f} | {prec:.4f} | {recall:.4f} |\\n\")\n",
        "    print(f\"Results saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zomk0vnNpA0R"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "wiki_path = \"/content/data/wiki-pages\"\n",
        "min_wiki_length = 10\n",
        "num_of_samples = 500\n",
        "topk = 15\n",
        "min_df = 2\n",
        "max_df = 0.8\n",
        "use_idf = True\n",
        "sublinear_tf = True\n",
        "\n",
        "# Set up the experiment name for logging\n",
        "exp_name = (\n",
        "    f\"len{min_wiki_length}_top{topk}_min_df={min_df}_\"\n",
        "    + f\"max_df={max_df}_{num_of_samples}s\"\n",
        ")\n",
        "if sublinear_tf:\n",
        "    exp_name = \"sublinearTF_\" + exp_name\n",
        "if not use_idf:\n",
        "    exp_name = \"no_idf_\" + exp_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh25hsnqpA0R"
      },
      "outputs": [],
      "source": [
        "# First time running this cell will 34 minutes using Google Colab.\n",
        "\n",
        "wiki_cache = \"wiki\"\n",
        "target_column = \"text\"\n",
        "\n",
        "wiki_cache_path = Path(f\"data/{wiki_cache}.pkl\")\n",
        "if wiki_cache_path.exists():\n",
        "    wiki_pages = pd.read_pickle(wiki_cache_path)\n",
        "else:\n",
        "    # You need to download `wiki-pages.zip` from the AICUP website\n",
        "    wiki_pages = jsonl_dir_to_df(wiki_path)\n",
        "    # wiki_pages are combined into one dataframe, so we need to reset the index\n",
        "    wiki_pages = wiki_pages.reset_index(drop=True)\n",
        "\n",
        "    # tokenize the text and keep the result in a new column `processed_text`\n",
        "    wiki_pages[\"processed_text\"] = wiki_pages[target_column].parallel_apply(\n",
        "        partial(tokenize, stopwords=stopwords)\n",
        "    )\n",
        "    # save the result to a pickle file\n",
        "    wiki_pages.to_pickle(wiki_cache_path, protocol=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnTgxNq_pA0S"
      },
      "outputs": [],
      "source": [
        "# Build the TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words = stopwords,\n",
        "    norm = None\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leKw7QkfpA0S"
      },
      "outputs": [],
      "source": [
        "wiki_pages = wiki_pages[\n",
        "    wiki_pages['processed_text'].str.len() > min_wiki_length\n",
        "]\n",
        "corpus = wiki_pages[\"processed_text\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXlOBSiqpA0S"
      },
      "outputs": [],
      "source": [
        "# Start to encode the corpus with TF-IDF\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# fit_transform will do the following two steps:\n",
        "# 1. fit: learn the vocabulary and idf from the corpus\n",
        "# 2. transform: transform the corpus into a vector space\n",
        "# Note the result is a sparse matrix, which contains lots of zeros for each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyaVIKi5pA0S"
      },
      "outputs": [],
      "source": [
        "train = load_json(\"/content/data/public_train.jsonl\")[:num_of_samples]\n",
        "train_df = pd.DataFrame(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7IkgJgdpA0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3877b2-f766-426a-d646-50e30a5b6931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.957 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.957 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "100%|██████████| 500/500 [13:43<00:00,  1.65s/it]\n"
          ]
        }
      ],
      "source": [
        "# Perform the prediction for document retrieval\n",
        "# If you use Google Colab, do not use parallel_apply due to the memory limit.\n",
        "\n",
        "# train_df[\"predictions\"] = train_df[\"claim\"].parallel_apply(\n",
        "train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n",
        "    partial(\n",
        "        get_pred_docs_sklearn,\n",
        "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "        vectorizer=vectorizer,\n",
        "        tf_idf_matrix=X,\n",
        "        wiki_pages=wiki_pages,\n",
        "        topk=topk,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKHxx5CppA0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3993e29-7318-4c89-f315-1c18b470a388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.471626486852051\n",
            "Recall: 0.7100668337510444\n",
            "Results saved to grid_search_results.md\n"
          ]
        }
      ],
      "source": [
        "precision = calculate_precision(train, train_df[\"predictions\"])\n",
        "recall = calculate_recall(train, train_df[\"predictions\"])\n",
        "results = {\n",
        "    \"exp_name\": exp_name,\n",
        "    \"f1_score\": 2.0 * precision * recall / (precision + recall),\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "}\n",
        "\n",
        "# This helps you to adjust the hyperparameters\n",
        "save_results_to_markdown(results, output_file=\"grid_search_results.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cjzq5nJpA0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cd8066-f287-44db-b10f-b1a5d7e55e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3942/3942 [1:41:45<00:00,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.4792312632441721\n",
            "Recall: 0.7030803946263642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Do not modify this cell.\n",
        "# This cell is for your scores on the training set.\n",
        "\n",
        "train = load_json(\"data/public_train.jsonl\")\n",
        "train_df = pd.DataFrame(train)\n",
        "\n",
        "# Perform the prediction for document retrieval\n",
        "train_df[\"predictions\"] = train_df[\"claim\"].progress_apply(\n",
        "    partial(\n",
        "        get_pred_docs_sklearn,\n",
        "        tokenizing_method=partial(tokenize, stopwords=stopwords),\n",
        "        vectorizer=vectorizer,\n",
        "        tf_idf_matrix=X,\n",
        "        wiki_pages=wiki_pages,\n",
        "        topk=topk,\n",
        "    )\n",
        ")\n",
        "precision = calculate_precision(train, train_df[\"predictions\"])\n",
        "recall = calculate_recall(train, train_df[\"predictions\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.16 ('hw3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16 (main, Mar  8 2023, 14:00:05) \n[GCC 11.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "7cbde34494a11b1624bcc1eb71dba5ceb4f0ff8d7a6c820b0d6fa32591a5e209"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}